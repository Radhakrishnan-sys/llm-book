{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca7a109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\" \n",
    "COMPREHENSIVE EDA TEMPLATE FOR TEXT DATA WITH VECTOR SEARCH PREPARATION\n",
    "========================================================================\n",
    "\n",
    "This notebook provides a complete template for:\n",
    "1. Exploratory Data Analysis (EDA) of text data\n",
    "2. Advanced cleaning and preprocessing\n",
    "3. Preparation for vector search with LLMs\n",
    "\n",
    "Each step includes detailed explanations of:\n",
    "- What the code does\n",
    "- Why it's important\n",
    "- How it processes the data\n",
    "\"\"\"\n",
    "\n",
    "# ======================\n",
    "# 1. SETUP & CONFIGURATION\n",
    "# ======================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import plotly.express as px\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Setup visualization style\n",
    "plt.style.use('ggplot')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Configuration\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "# ======================\n",
    "# 2. DATA LOADING\n",
    "# ======================\n",
    "\"\"\"\n",
    "WHY: Proper data loading is crucial for reproducibility and handling different data formats.\n",
    "HOW: We'll use pandas to load the data with error handling.\n",
    "\"\"\"\n",
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"thedevastator/text-classification-for-qa-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "best_books=pd.read_csv(f\"{path}/test.csv\")\n",
    "try:\n",
    "    # Load your dataset - update this path as needed\n",
    "    df = pd.read_csv('your_dataset.csv')  # Replace with your actual data loading code\n",
    "    print(\"‚úÖ Data loaded successfully!\")\n",
    "    \n",
    "    # Display basic loading info\n",
    "    print(f\"\\nüìä Dataset Dimensions: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "    print(f\"\\nüîç First 5 rows:\")\n",
    "    display(df.head())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading data: {str(e)}\")\n",
    "    # If using the hate speech dataset from your example:\n",
    "    print(\"\\n‚ö†Ô∏è Trying sample data...\")\n",
    "    import requests\n",
    "    from io import StringIO\n",
    "    url = \"https://raw.githubusercontent.com/t-davidson/hate-speech-and-offensive-language/master/data/labeled_data.csv\"\n",
    "    content = requests.get(url).content\n",
    "    df = pd.read_csv(StringIO(content.decode('utf-8')))\n",
    "    print(\"‚úÖ Sample hate speech data loaded as fallback!\")\n",
    "    display(df.head())\n",
    "\n",
    "# ======================\n",
    "# 3. BASIC EDA\n",
    "# ======================\n",
    "\"\"\"\n",
    "WHY: Understanding the raw data structure before any processing.\n",
    "HOW: We'll examine metadata, distributions, and basic statistics.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üßê BASIC EXPLORATORY DATA ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 3.1 Dataset Metadata\n",
    "print(\"\\nüìå DATASET METADATA\")\n",
    "print(\"-\"*40)\n",
    "print(f\"‚Ä¢ Column names: {list(df.columns)}\")\n",
    "print(f\"‚Ä¢ Data types:\\n{df.dtypes}\")\n",
    "print(f\"‚Ä¢ Missing values:\\n{df.isna().sum()}\")\n",
    "print(f\"‚Ä¢ Duplicate rows: {df.duplicated().sum()}\")\n",
    "\n",
    "# 3.2 Target Variable Analysis (if exists)\n",
    "if 'class' in df.columns:\n",
    "    print(\"\\nüéØ TARGET VARIABLE ANALYSIS ('class')\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    # Value counts with percentages\n",
    "    target_dist = df['class'].value_counts(normalize=True).mul(100).round(1)\n",
    "    display(target_dist)\n",
    "    \n",
    "    # Visual distribution\n",
    "    plt.figure(figsize=(10,5))\n",
    "    ax = sns.countplot(data=df, x='class')\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(f'{p.get_height()}\\n({p.get_height()/len(df)*100:.1f}%)', \n",
    "                   (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                   ha='center', va='center', xytext=(0,10), textcoords='offset points')\n",
    "    plt.title('Class Distribution with Percentages')\n",
    "    plt.show()\n",
    "\n",
    "# 3.3 Text Length Analysis\n",
    "print(\"\\nüìè TEXT LENGTH ANALYSIS\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Calculate text statistics\n",
    "df['text_length'] = df['tweet'].apply(len)\n",
    "df['word_count'] = df['tweet'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Plot distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15,5))\n",
    "sns.histplot(df['text_length'], bins=50, ax=axes[0], kde=True)\n",
    "axes[0].set_title('Character Length Distribution')\n",
    "sns.histplot(df['word_count'], bins=50, ax=axes[1], kde=True)\n",
    "axes[1].set_title('Word Count Distribution')\n",
    "plt.show()\n",
    "\n",
    "# Show text length by class (if available)\n",
    "if 'class' in df.columns:\n",
    "    plt.figure(figsize=(12,6))\n",
    "    sns.boxplot(data=df, x='class', y='word_count')\n",
    "    plt.title('Word Count Distribution by Class')\n",
    "    plt.show()\n",
    "\n",
    "# ======================\n",
    "# 4. TEXT PREPROCESSING\n",
    "# ======================\n",
    "\"\"\"\n",
    "WHY: Clean text data improves model performance and vector search quality.\n",
    "HOW: We'll implement a comprehensive cleaning pipeline with explanations.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üßπ TEXT CLEANING & PREPROCESSING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Install necessary packages\n",
    "!pip install -U spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize NLP tools\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tqdm.pandas()  # Enable progress bars\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Comprehensive text cleaning function with explanations for each step.\n",
    "    \n",
    "    Parameters:\n",
    "    text (str): Raw input text\n",
    "    \n",
    "    Returns:\n",
    "    str: Cleaned text ready for vectorization\n",
    "    \"\"\"\n",
    "    # 1. Lowercasing (standardizes text)\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. Remove URLs (common in social media text)\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # 3. Remove user mentions (@) and hashtags (#)\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "    \n",
    "    # 4. Remove special characters and numbers (keep only letters)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # 5. Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # 6. Lemmatization (reduce words to base form)\n",
    "    doc = nlp(text)\n",
    "    text = ' '.join([token.lemma_ for token in doc])\n",
    "    \n",
    "    # 7. Remove stopwords (common words that add little meaning)\n",
    "    text = ' '.join([word for word in text.split() \n",
    "                    if word not in stop_words and len(word) > 2])\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply cleaning with progress bar\n",
    "print(\"üîÑ Cleaning text... (this may take a few minutes)\")\n",
    "df['cleaned_text'] = df['tweet'].progress_apply(clean_text)\n",
    "\n",
    "# Show before/after examples\n",
    "print(\"\\nüÜö BEFORE/AFTER CLEANING EXAMPLES\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nOriginal ({i}): {df['tweet'].iloc[i]}\")\n",
    "    print(f\"Cleaned ({i}): {df['cleaned_text'].iloc[i]}\")\n",
    "\n",
    "# ======================\n",
    "# 5. ADVANCED EDA\n",
    "# ======================\n",
    "\"\"\"\n",
    "WHY: Deeper understanding of text patterns and relationships.\n",
    "HOW: We'll use visualization and statistical analysis of cleaned text.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üîç ADVANCED TEXT ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 5.1 Word Clouds by Class\n",
    "print(\"\\n‚òÅÔ∏è WORD CLOUDS BY CLASS\")\n",
    "if 'class' in df.columns:\n",
    "    for label in sorted(df['class'].unique()):\n",
    "        text = ' '.join(df[df['class']==label]['cleaned_text'])\n",
    "        wordcloud = WordCloud(width=800, height=400, \n",
    "                            background_color='white',\n",
    "                            colormap='Reds' if label == 1 else 'Blues',\n",
    "                            max_words=100).generate(text)\n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.imshow(wordcloud)\n",
    "        plt.title(f'Class {label} - Most Frequent Words')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# 5.2 N-gram Analysis\n",
    "print(\"\\nüìä N-GRAM ANALYSIS (Most Common Phrases)\")\n",
    "\n",
    "def plot_top_ngrams(series, n=20, ngram_range=(1,1), title=\"\"):\n",
    "    \"\"\"\n",
    "    Analyze and visualize most common n-grams.\n",
    "    \n",
    "    Parameters:\n",
    "    series: Text data to analyze\n",
    "    n: Number of top n-grams to show\n",
    "    ngram_range: Range of n-grams (1=unigram, 2=bigram, etc.)\n",
    "    title: Plot title\n",
    "    \"\"\"\n",
    "    vec = CountVectorizer(ngram_range=ngram_range, \n",
    "                         max_features=2000).fit(series)\n",
    "    bag_of_words = vec.transform(series)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) \n",
    "                 for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    plt.figure(figsize=(10,5))\n",
    "    sns.barplot(x=[x[1] for x in words_freq[:n]], \n",
    "                y=[x[0] for x in words_freq[:n]])\n",
    "    plt.title(f'Top {n} {\" \".join([str(x) for x in ngram_range])}-grams: {title}')\n",
    "    plt.show()\n",
    "\n",
    "# Unigrams\n",
    "plot_top_ngrams(df['cleaned_text'], ngram_range=(1,1), \n",
    "               title=\"All Text\")\n",
    "\n",
    "# Bigrams\n",
    "plot_top_ngrams(df['cleaned_text'], ngram_range=(2,2), \n",
    "               title=\"All Text\")\n",
    "\n",
    "# Class-specific n-grams (if available)\n",
    "if 'class' in df.columns:\n",
    "    for label in sorted(df['class'].unique()):\n",
    "        class_text = df[df['class']==label]['cleaned_text']\n",
    "        plot_top_ngrams(class_text, ngram_range=(1,2), \n",
    "                       title=f\"Class {label}\")\n",
    "\n",
    "# ======================\n",
    "# 6. VECTOR SEARCH PREPARATION\n",
    "# ======================\n",
    "\"\"\"\n",
    "WHY: Prepare text embeddings for efficient similarity search with LLMs.\n",
    "HOW: We'll generate sentence embeddings and visualize the vector space.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üîÆ VECTOR SEARCH PREPARATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 6.1 Install and setup sentence transformers\n",
    "!pip install -U sentence-transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "print(\"\\nüîß Setting up sentence transformer model...\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\"\"\"\n",
    "Model choice explanation:\n",
    "- 'all-MiniLM-L6-v2': Good balance between speed (384-dim) and quality\n",
    "- Other options: \n",
    "  - 'all-mpnet-base-v2' (higher quality, 768-dim)\n",
    "  - 'multi-qa-mpnet-base-dot-v1' (optimized for semantic search)\n",
    "\"\"\"\n",
    "\n",
    "# 6.2 Generate embeddings (using sample for demonstration)\n",
    "sample_size = min(1000, len(df))  # Adjust based on your resources\n",
    "sample_texts = df['cleaned_text'].sample(sample_size, random_state=42).tolist()\n",
    "\n",
    "print(f\"\\nüîÑ Generating embeddings for {sample_size} samples...\")\n",
    "embeddings = model.encode(sample_texts, \n",
    "                         batch_size=32, \n",
    "                         show_progress_bar=True,\n",
    "                         convert_to_numpy=True)\n",
    "\n",
    "# 6.3 Dimensionality reduction for visualization\n",
    "print(\"\\nüé® Visualizing embeddings with t-SNE...\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "embeddings_2d = tsne.fit_transform(embeddings)\n",
    "\n",
    "# Create visualization\n",
    "plt.figure(figsize=(12,10))\n",
    "if 'class' in df.columns:\n",
    "    classes = df.loc[sample_texts.index, 'class']\n",
    "    scatter = plt.scatter(embeddings_2d[:,0], embeddings_2d[:,1], \n",
    "                         c=classes, alpha=0.6, cmap='viridis')\n",
    "    plt.colorbar(scatter).set_label('Class')\n",
    "else:\n",
    "    plt.scatter(embeddings_2d[:,0], embeddings_2d[:,1], alpha=0.6)\n",
    "plt.title('t-SNE Visualization of Text Embeddings')\n",
    "plt.xlabel('t-SNE Dimension 1')\n",
    "plt.ylabel('t-SNE Dimension 2')\n",
    "plt.show()\n",
    "\n",
    "# 6.4 Save embeddings for future use\n",
    "print(\"\\nüíæ Saving embeddings for vector search...\")\n",
    "embedding_df = pd.DataFrame({\n",
    "    'text': sample_texts,\n",
    "    'embedding': list(embeddings)\n",
    "})\n",
    "\n",
    "if 'class' in df.columns:\n",
    "    embedding_df['class'] = df.loc[sample_texts.index, 'class'].values\n",
    "\n",
    "# Example: Save to file\n",
    "embedding_df.to_pickle('text_embeddings.pkl')\n",
    "print(\"‚úÖ Embeddings saved to 'text_embeddings.pkl'\")\n",
    "\n",
    "# ======================\n",
    "# 7. NEXT STEPS\n",
    "# ======================\n",
    "\"\"\"\n",
    "Suggestions for what to do next with your prepared data:\n",
    "1. Vector Search: Use FAISS or Annoy for efficient similarity search\n",
    "2. Classification: Train a model using the embeddings as features\n",
    "3. Clustering: Group similar texts using K-Means or HDBSCAN\n",
    "4. Topic Modeling: Discover latent topics with LDA or BERTopic\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üöÄ NEXT STEPS SUGGESTIONS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "next_steps = \"\"\"\n",
    "Recommended next steps for your project:\n",
    "\n",
    "1. üéØ VECTOR SEARCH IMPLEMENTATION:\n",
    "   - Install: !pip install faiss-cpu\n",
    "   - Build index: \n",
    "     import faiss\n",
    "     index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "     index.add(embeddings)\n",
    "\n",
    "2. ü§ñ CLASSIFICATION MODEL:\n",
    "   from sklearn.ensemble import RandomForestClassifier\n",
    "   clf = RandomForestClassifier()\n",
    "   clf.fit(embeddings, df.loc[sample_texts.index, 'class'])\n",
    "\n",
    "3. üåÄ CLUSTERING ANALYSIS:\n",
    "   from sklearn.cluster import KMeans\n",
    "   kmeans = KMeans(n_clusters=3)\n",
    "   clusters = kmeans.fit_predict(embeddings)\n",
    "\n",
    "4. üìö TOPIC MODELING:\n",
    "   !pip install bertopic\n",
    "   from bertopic import BERTopic\n",
    "   topic_model = BERTopic()\n",
    "   topics, _ = topic_model.fit_transform(sample_texts)\n",
    "\"\"\"\n",
    "\n",
    "print(next_steps)\n",
    "print(\"\\n‚ú® EDA and Vector Search Preparation Complete! ‚ú®\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
